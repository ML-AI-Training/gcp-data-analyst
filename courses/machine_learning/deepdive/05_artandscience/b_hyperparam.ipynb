{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4f3CKqFUqL2-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyperparameter tuning with Cloud ML Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives:**\n",
    "  * Improve the accuracy of a model by hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'cloud-training-demos' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'cloud-training-demos-ml' # REPLACE WIHT YOUR BUCKET NAME\n",
    "REGION = 'us-central1' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "os.environ['TFVERSION'] = '1.4'  # Tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TjLjL9IU80G"
   },
   "source": [
    "## Create command-line program\n",
    "\n",
    "In order to submit to Cloud ML Engine, we need to create a distributed training program. Let's convert our housing example to fit that paradigm, using the Estimators API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf trainer\n",
    "mkdir trainer\n",
    "touch trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing trainer/house.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile trainer/house.py\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "def train(output_dir, batch_size, learning_rate):\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  \n",
    "  # Read dataset and split into train and eval\n",
    "  df = pd.read_csv(\"https://storage.googleapis.com/ml_universities/california_housing_train.csv\", sep=\",\")\n",
    "  df['num_rooms'] = df['total_rooms'] / df['households']\n",
    "  msk = np.random.rand(len(df)) < 0.8\n",
    "  traindf = df[msk]\n",
    "  evaldf = df[~msk]\n",
    "\n",
    "  # Train and eval input functions\n",
    "  SCALE = 100000\n",
    "  \n",
    "  train_input_fn = tf.estimator.inputs.pandas_input_fn(x = traindf[[\"num_rooms\"]],\n",
    "                                                       y = traindf[\"median_house_value\"] / SCALE,  # note the scaling\n",
    "                                                       num_epochs = 1,\n",
    "                                                       batch_size = batch_size, # note the batch size\n",
    "                                                       shuffle = True)\n",
    "  \n",
    "  eval_input_fn = tf.estimator.inputs.pandas_input_fn(x = evaldf[[\"num_rooms\"]],\n",
    "                                                      y = evaldf[\"median_house_value\"] / SCALE,  # note the scaling\n",
    "                                                      num_epochs = 1,\n",
    "                                                      batch_size = len(evaldf),\n",
    "                                                      shuffle=False)\n",
    "  \n",
    "  # Define feature columns\n",
    "  features = [tf.feature_column.numeric_column('num_rooms')]\n",
    "  \n",
    "  def train_and_evaluate(output_dir):\n",
    "    # Compute appropriate number of steps\n",
    "    num_steps = (len(traindf) / batch_size) / learning_rate  # if learning_rate=0.01, hundred epochs\n",
    "\n",
    "    # Create custom optimizer\n",
    "    myopt = tf.train.FtrlOptimizer(learning_rate = learning_rate) # note the learning rate\n",
    "\n",
    "    # Create rest of the estimator as usual\n",
    "    estimator = tf.estimator.LinearRegressor(model_dir = output_dir, \n",
    "                                             feature_columns = features, \n",
    "                                             optimizer = myopt)\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn = train_input_fn,\n",
    "                                        max_steps = num_steps)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn = eval_input_fn,\n",
    "                                      steps = None)\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "  # Run the training\n",
    "  shutil.rmtree(output_dir, ignore_errors=True) # start fresh each time\n",
    "  train_and_evaluate(output_dir)\n",
    "    \n",
    "if __name__ == '__main__' and \"get_ipython\" not in dir():\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument(\n",
    "      '--learning_rate',\n",
    "      type = float, \n",
    "      default = 0.01\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--batch_size',\n",
    "      type = int, \n",
    "      default = 30\n",
    "  ),\n",
    "  parser.add_argument(\n",
    "      '--output_dir',\n",
    "      help = 'GCS location to write checkpoints and export models',\n",
    "      required = True\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      '--job-dir',\n",
    "      help = 'this model ignores this field, but it is required by gcloud',\n",
    "      default = 'junk'\n",
    "  )\n",
    "  args = parser.parse_args()\n",
    "  train(args.output_dir, args.batch_size, args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf house_trained\n",
    "gcloud ml-engine local train \\\n",
    "    --module-name=trainer.house \\\n",
    "    --job-dir=output_dir \\\n",
    "    --package-path=$(pwd)/trainer \\\n",
    "    -- \\\n",
    "    --output_dir=house_trained \\\n",
    "    --batch_size=30 \\\n",
    "    --learning_rate=0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create hyperparam.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperparam.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MINIMIZE\n",
    "    maxTrials: 5\n",
    "    maxParallelTrials: 1\n",
    "    hyperparameterMetricTag: average_loss\n",
    "    params:\n",
    "    - parameterName: batch_size\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 512\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: learning_rate\n",
    "      type: DOUBLE\n",
    "      minValue: 0.001\n",
    "      maxValue: 0.1\n",
    "      scaleType: UNIT_LOG_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/house_trained   # CHANGE bucket name appropriately\n",
    "gsutil rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training house_$(date -u +%y%m%d_%H%M%S) \\\n",
    "   --config=hyperparam.yaml \\\n",
    "   --module-name=trainer.house \\\n",
    "   --package-path=$(pwd)/trainer \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --runtime-version=$TFVERSION \\\n",
    "   -- \\\n",
    "   --output_dir=$OUTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!gcloud ml-engine jobs describe house_170820_034011 # CHANGE jobId appropriately"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "first_steps_with_tensor_flow.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
